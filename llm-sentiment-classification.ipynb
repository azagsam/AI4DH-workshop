{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf0d47-3dfb-4b47-8d84-77d2342011f0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "  transformers==4.57.3 \\\n",
    "  bitsandbytes==0.49.0 \\\n",
    "  accelerate==1.12.0 \\\n",
    "  sentence-transformers==5.1.2 \\\n",
    "  faiss-cpu==1.13.1 \\\n",
    "  datasets==4.0.0 \\\n",
    "  evaluate==0.4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966b9f0-a6ac-433f-8be6-57135b513ad9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# Initialize the model with quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Set to True for 4-bit quantization or False for 8-bit\n",
    "    bnb_4bit_use_double_quant=True,  # Optional: Improves stability in 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Optional: Use 'nf4' for better accuracy or 'fp4' for faster computation\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Optional: use float16 for better performance on newer GPUs\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config  # Pass the quantization configuration\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095a5b6-8f0b-4dd2-a89b-e1bc5d6bca75",
   "metadata": {},
   "source": [
    "## Classification with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592f41f-ed54-4ba4-bee3-81193e6dd82d",
   "metadata": {},
   "source": [
    "So far, we have taken a look at using LLMs for tasks such as *open-ended question answering* and *machine translation* - both are what we call **generative tasks**. In simplified terms this means that we expect the model to generate outputs that may be very different depending on the input context we provide. The answer to *\"Give me a short introduction to large language models.\"* will be structurally and semantically very different to *the German translation of an input sentence*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb28c3-5382-4d5c-8ff5-c3753256f272",
   "metadata": {},
   "source": [
    "Another type of tasks we can tackle with large language models are **discriminative (classification) tasks**.  \n",
    "Given an input text, the goal in classification tasks is to assign it one* out of a small set of possible labels. One example of a classification task we will take a look at is called **sentiment classification**.\n",
    "\n",
    "\\* For simplicity, we are only dealing with single-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beea0f9-d6f7-4770-a3df-2c692255289e",
   "metadata": {},
   "source": [
    "In sentiment classification, the goal is to identify the emotional polarity expressed in a text. Usually, a text may have a **positive**, **negative** or **neutral** sentiment.\n",
    "\n",
    "The way we are going to approach the discriminative task is going to be very similar to the previous (generative) tasks, reusing much of the code from the notebook *llm-machine-translation*. **Very importantly however, the answer we expect from the model is significantly less free-form than before**. To this extent, we are going to specify the valid answer options for the LLM in its prompt, asking it to respond with either \"Positive\", \"Negative\", or \"Neutral\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b8970-5b5c-48ab-b92a-7fced0e1ffb4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = (\"Classify the sentiment of the following text as 'Positive', 'Negative', or 'Neutral'.\\n\\n\"\n",
    "          \"Today is a good day.\")\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625164b-2612-414f-9696-aa92a33d502d",
   "metadata": {},
   "source": [
    "Constraining the output structure through the prompt already works quite well, guiding the model to answer with one of the valid answers.\n",
    "By providing constraints in the prompt, we are attempting to limit the possible next words generated by the LLM to only *Positive*, *Negative*, and *Neutral*. In technical terms, we are trying to achieve probability zero for all other words.\n",
    "\n",
    "In practice, however, guiding the model through the prompt alone is not a robust way to limit the valid answers. Numerous next words maintain a small yet nonzero probability, meaning the LLM may occasionally answer with additional options.\n",
    "For example, instead of generating a single-word response \n",
    "#### *Positive* \n",
    "the model might output a full sentence response \n",
    "#### *The sentiment of the given text is Positive*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70650d3f-2b89-4534-95e9-15133260e0f6",
   "metadata": {},
   "source": [
    "# Guided decoding\n",
    "A more robust way to constrain the set of possible answers is by using **guided decoding**. Broadly speaking, [guided decoding](https://huggingface.co/blog/nmmursit/guided-decoding) is a set of methods for enforcing that the model response follows a structure. Simplified for our purpose (classification), the idea of guided decoding is to set the probability of all invalid (unspecified) answers to 0. This means that only the valid answers may be generated by the LLM.\n",
    "\n",
    "To demonstrate this in practice, we will use **vLLM**, a convenient library for working with LLMs efficiently. We take a more detailed look at the library in the notebook *faster-inference-vllm.ipynb*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ad655-bc01-45c8-9d38-f3f3091ef962",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# the vLLM library uses its own methods for working with the models, so we have to reload the model\n",
    "del model\n",
    "!pip install vllm==0.10.2 bitsandbytes==0.46.1 transformers==4.57.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d7c8a-9a72-4ca0-86b0-887eeb5cd974",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "model = LLM(\n",
    "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    quantization=\"bitsandbytes\",\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a50ac-3e9d-49b5-8c51-ddaf8d9f7d5c",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 01-01 13:19:14 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "guided_decoding_params = GuidedDecodingParams(choice=[\"Positive\", \"Negative\", \"Neutral\"])\n",
    "sampling_params = SamplingParams(guided_decoding=guided_decoding_params)\n",
    "outputs = model.chat(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "\n",
    "response = outputs[0].outputs[0].text\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25899b12-44b6-4a66-9ecd-1f7d202d10f7",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. How well does the model work using the simple prompt? Gather a small sample of positive, negative, and neutral texts. Classify the texts with a LLM, and compare the predicted answers to the correct ones.\n",
    "2. Try to improve the classification accuracy by modifying the prompt. Two things you can try doing is:  \n",
    "- including a more detailed definition of the sentiment classification task;\n",
    "- including demonstrations of input texts and their sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13269186-d6ac-4292-b7e0-bd32b9210b54",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
