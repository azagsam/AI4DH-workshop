{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966b9f0-a6ac-433f-8be6-57135b513ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# Initialize the model with quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Set to True for 4-bit quantization or False for 8-bit\n",
    "    bnb_4bit_use_double_quant=True,  # Optional: Improves stability in 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Optional: Use 'nf4' for better accuracy or 'fp4' for faster computation\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Optional: use float16 for better performance on newer GPUs\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config  # Pass the quantization configuration\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095a5b6-8f0b-4dd2-a89b-e1bc5d6bca75",
   "metadata": {},
   "source": [
    "## Classification with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592f41f-ed54-4ba4-bee3-81193e6dd82d",
   "metadata": {},
   "source": [
    "So far, we have taken a look at using LLMs for tasks such as *open-ended question answering* and *machine translation* - both are what we call **generative tasks**. In simplified terms this means that we expect the model to generate outputs that may be very different depending on the input context we provide. The answer to *\"Give me a short introduction to large language models.\"* will be structurally and semantically very different to *the German translation of an input sentence*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb28c3-5382-4d5c-8ff5-c3753256f272",
   "metadata": {},
   "source": [
    "Another type of tasks we can tackle with large language models are **discriminative (classification) tasks**.  \n",
    "Given an input text, the goal in classification tasks is to assign it one* out of a small set of possible labels. One example of a classification task we will take a look at is called **sentiment classification**.\n",
    "\n",
    "\\* For simplicity, we are only dealing with single-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beea0f9-d6f7-4770-a3df-2c692255289e",
   "metadata": {},
   "source": [
    "In sentiment classification, the goal is to identify the emotional polarity expressed in a text. Usually, a text may have a **positive**, **negative** or **neutral** sentiment.\n",
    "\n",
    "The way we are going to approach the discriminative task is going to be very similar to the previous (generative) tasks, reusing much of the code from the notebook *llm-machine-translation*. **Very importantly however, the answer we expect from the model is significantly less free-form than before**. To this extent, we are going to specify the valid answer options for the LLM in its prompt, asking it to respond with either \"Positive\", \"Negative\", or \"Neutral\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b8970-5b5c-48ab-b92a-7fced0e1ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"Classify the sentiment of the following text as 'Positive', 'Negative', or 'Neutral'.\\n\\n\"\n",
    "          \"Today is a good day.\")\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625164b-2612-414f-9696-aa92a33d502d",
   "metadata": {},
   "source": [
    "Constraining the output structure through the prompt already works quite well, guiding the model to answer with one of the valid answers.\n",
    "By providing constraints in the prompt, we are attempting to limit the possible next words generated by the LLM to only \"Positive\", \"Negative\", and \"Neutral\". In technical terms, we are trying to achieve probability zero for all other words.\n",
    "\n",
    "In practice, however, some other words maintain a small yet nonzero probability, meaning the LLM may occasionally answer with additional options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70650d3f-2b89-4534-95e9-15133260e0f6",
   "metadata": {},
   "source": [
    "A more robust way to constrain the set of possible answers is by using **guided decoding**. In simple terms, the idea of guided decoding is to set the probability of all invalid (unspecified) answers to 0. This means that only the valid answers may be generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433ef8e-feb1-4715-bdc6-5622a6924bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    generation_config=GenerationConfig(\n",
    "        guided_decoding={\n",
    "            \"type\": \"choice\",\n",
    "            \"choices\": [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "        },\n",
    "        max_new_tokens=5,\n",
    "        temperature=0.6,\n",
    "    )\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25899b12-44b6-4a66-9ecd-1f7d202d10f7",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. How well does the model work using the simple prompt? Gather a small sample of positive, negative, and neutral texts. Classify the texts with a LLM, and compare the predicted answers to the correct ones.\n",
    "2. Try to improve the classification accuracy by modifying the prompt. Two things you can try doing is:  \n",
    "- including a more detailed definition of the sentiment classification task;\n",
    "- including demonstrations of input texts and their sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13269186-d6ac-4292-b7e0-bd32b9210b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
