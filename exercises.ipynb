{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbe1024",
   "metadata": {},
   "source": [
    "## Exercise 1: First steps with a Large Language Model (GaMS)\n",
    "\n",
    "In this exercise, you will get hands-on experience with a **large language model (LLM)** that supports Slovenian and English. The goal is to understand what an LLM can do, how it responds to different types of prompts, and how inference settings influence its behavior.\n",
    "\n",
    "We will use **GaMS-2B-Instruct**, a Slovenian-centric instruction-tuned model, and interact with it through simple prompting tasks.\n",
    "\n",
    "### Model and resources\n",
    "- **Model & example code**: https://huggingface.co/cjvt/GaMS-2B-Instruct\n",
    "\n",
    "---\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Ask general questions**\n",
    "   - Prompt the model with a few simple questions in Slovene and in English.\n",
    "   - Observe fluency, factuality, and language switching behavior.\n",
    "\n",
    "2. **Machine translation**\n",
    "   - Ask GaMS to translate the following Slovenian sentence into English:\n",
    "     > *Vlada Republike Slovenije je organ izvršilne oblasti, hkrati pa je tudi najvišji organ državne uprave.*\n",
    "   - Try to translate into other languages. Does it work for an arbitrary language?  \n",
    "\n",
    "3. **Text summarization**\n",
    "   - Find a short news article on the web.\n",
    "   - Paste the article into the prompt and ask GaMS to produce a concise summary.\n",
    "   - Optionally specify constraints (e.g. length, bullet points, one-sentence summary).\n",
    "\n",
    "4. **Inference hyperparameters**\n",
    "   - Change generation parameters such as:\n",
    "     - `temperature`\n",
    "     - `top_p`\n",
    "     - `max_new_tokens`\n",
    "   - Compare outputs across different settings for:\n",
    "     - translation\n",
    "     - summarization\n",
    "   - Note differences in determinism, verbosity, and creativity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c5c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d189cf5d",
   "metadata": {},
   "source": [
    "## Exercise 2: Using Large Language Models via APIs\n",
    "\n",
    "In this exercise, you will learn how to interact with **large language models through APIs**, which is the most common way LLMs are used in research and industry. Instead of running a model locally, you will send requests to a remote model hosted by a provider and receive generated text as a response.\n",
    "\n",
    "We will focus on the general principles of API-based LLM usage, which transfer across providers and models.\n",
    "\n",
    "### Resources and starter code\n",
    "- **API access**: All participants will be provided with an **API key** for the duration of the workshop.\n",
    "- You will find **starter notebooks with example code** for using commercial LLMs in the provided **OpenAI notebook**.\n",
    "- The notebook demonstrates:\n",
    "  - API authentication\n",
    "  - Sending prompts to a model\n",
    "  - Controlling inference parameters\n",
    "  - Processing model outputs programmatically\n",
    "\n",
    "---\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Run the starter notebook**\n",
    "   - Open the provided OpenAI notebook.\n",
    "   - Verify that API access is working by running a simple prompt.\n",
    "\n",
    "2. **Ask general questions**\n",
    "   - Send a few questions to the model in English or Slovene.\n",
    "   - Observe response quality, latency, and consistency.\n",
    "\n",
    "3. **Machine translation**\n",
    "   - Use the API to translate the following Slovenian sentence into English:\n",
    "     > *Vlada Republike Slovenije je organ izvršilne oblasti, hkrati pa je tudi najvišji organ državne uprave.*\n",
    "\n",
    "4. **Text summarization**\n",
    "   - Provide a short news article as input.\n",
    "   - Request a concise summary using the API.\n",
    "   - Optionally constrain the output (e.g. maximum length, bullet points).\n",
    "\n",
    "5. **Experiment with parameters**\n",
    "   - Modify generation parameters such as:\n",
    "     - `temperature`\n",
    "     - `max_tokens`\n",
    "     - `top_p`\n",
    "   - Compare outputs across different configurations and tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33545e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97be3b19",
   "metadata": {},
   "source": [
    "## Exercise 3: Semantic Clustering with Text Embeddings \n",
    "\n",
    "In this exercise, you will evaluate how **different text embedding models and clustering algorithms** affect cluster quality and interpretability.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "#### 1. Corpus preparation\n",
    "- Find **~10 short texts**, such as:\n",
    "  - news headlines,\n",
    "  - short news paragraphs,\n",
    "  - abstracts or encyclopedia snippets.\n",
    "- Ensure the texts span **at least 3–5 domains**.\n",
    "- Save them on disk or into a list. \n",
    "\n",
    "#### 2. Embedding model selection\n",
    "- Use the **Hugging Face ecosystem** for embedding models. Suggestions: https://huggingface.co/Qwen/Qwen3-Embedding-0.6B or https://huggingface.co/BAAI/bge-m3  \n",
    "- You will also find relevant models on https://sbert.net/\n",
    "\n",
    "#### 3. Embedding generation\n",
    "- Encode all texts using each selected model.\n",
    "\n",
    "#### 4. Clustering algorithm selection\n",
    "- Perform clustering using standard libraries (e.g. `scikit-learn`). Apply **at least two** different clustering algorithms, such as:\n",
    "  - **k-means** (fixed number of clusters),\n",
    "  - **DBSCAN** (density-based, no fixed *k*).\n",
    "\n",
    "#### 5. Results comparison\n",
    "- For each *(embedding model × clustering algorithm)* combination:\n",
    "  - inspect cluster assignments,\n",
    "- Compare results along:\n",
    "  - semantic coherence,\n",
    "  - separation between clusters,\n",
    "  - sensitivity to hyperparameters.\n",
    "- Try to adjust clustering hyperparameters to get better results. \n",
    "\n",
    "#### 6. Improve results\n",
    "- Experiment with dimensonality reduction technique, e.g. PCA, and report if they improve the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22d9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5507c56",
   "metadata": {},
   "source": [
    "## Exercise 4: Advanced Inference — Comparing Hugging Face and vLLM Performance\n",
    "\n",
    "This exercise is intended for **advanced users** who want to understand performance trade-offs between different LLM inference frameworks. You will compare standard Hugging Face–based inference with **vLLM**, a high-throughput inference engine designed for efficient serving of large language models.\n",
    "\n",
    "The focus is on **speed, throughput, and resource utilization**, rather than model quality.\n",
    "\n",
    "---\n",
    "\n",
    "### Setup\n",
    "\n",
    "- Use the **same model** for both frameworks (e.g. GaMS or another comparable open model).\n",
    "- Ensure identical hardware and similar settings where possible.\n",
    "- Disable unnecessary logging and debugging outputs to avoid skewing timings.\n",
    "\n",
    "---\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Select an appropriate dataset (at least 1000 records) from https://huggingface.co/datasets and download it.**\n",
    "   - You can also use dataset provided in other notebooks. \n",
    "\n",
    "2. **Baseline: Hugging Face inference**\n",
    "   - Load appropriate model using the Hugging Face `transformers` library.\n",
    "   - Run inference for a fixed set of prompts (max. 10) from the selected dataset.\n",
    "   - Measure total generation time.\n",
    "\n",
    "3. **vLLM inference**\n",
    "   - Load the same model using vLLM. Consult the notebook on faster inference with vLLM. \n",
    "   - Use the same prompts and generation parameters.\n",
    "   - Measure total generation time.\n",
    "\n",
    "4. **Results comparison**\n",
    "   - Create a small table comparing both libraries. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397d61f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
